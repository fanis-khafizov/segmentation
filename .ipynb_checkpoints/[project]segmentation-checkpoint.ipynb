{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AGxjYy4yJvPX"
   },
   "source": [
    "<p style=\"align: center;\"><img src=\"https://static.tildacdn.com/tild6636-3531-4239-b465-376364646465/Deep_Learning_School.png\", width=500 ></p>\n",
    "\n",
    "<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cHdRPd9wJvPZ"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h8ZhKT5YJvPa"
   },
   "source": [
    "<h2 style=\"text-align: center;\"><b>Neural   Image Segmentation</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "30TzvdNwJvPb"
   },
   "source": [
    "<p style=\"text-align: center;\"><i>руководитель проекта: Илья Захаркин (Сколтех, ФИВТ МФТИ) | @ilyazakharkin</i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mQH5qCl5JvPc"
   },
   "source": [
    "<p style=\"text-align: center;\"><img src=\"https://he-s3.s3.amazonaws.com/media/uploads/67d854f.png\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "At1_ifZtJvPc"
   },
   "source": [
    "<h2 style=\"text-align: center;\"><b>Этапы работы</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2AQklPkpJvPd"
   },
   "source": [
    "Проект подразумевает выполнение всеми участниками пунктов 1-2, и далее выбор каждым слушателем своего сценария -- 1 или 2. Можно сделать оба сценария, то есть обучить модель на своих данных и её же встроить в демо, но для этого стоит быть готовым к удвоению затрачиваемого на проект времени."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wkEwp35fJvPe"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Общий этап работы</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x0yQIgyGJvPe"
   },
   "source": [
    "**1). Выбор фреймворка/библиотеки для использования сегментации**\n",
    "\n",
    "В выборе фреймворка предоставляется свобода, однако лично я смогу помочь в большей степени со следующими (перечислены в порядке моей личной рекомендации):\n",
    "- [`torchvision.models.segmentation`](https://pytorch.org/docs/stable/_modules/torchvision/models/segmentation/segmentation.html): \"нативные\" модели для сегментации прямо из PyTorch. Примеры использования есть прямо на [занятии DLSchool по практике CV](https://www.youtube.com/watch?v=XSPYe4-y4HE), можно начать именно с того нойтбука и убедитьс, что все работает. **Hint**: если у вас на компьютере нет GPU, лучше с самого начала делать все в Google Colab.\n",
    "- `detectron2`: краткая информация есть в конце [занятия DLSchool по практике CV](https://www.youtube.com/watch?v=XSPYe4-y4HE), можно начать с него. Лучше самостоятелньо изучить [официальный репозиторий](https://github.com/facebookresearch/detectron2) и уже с ним работать в дальнейшем (там есть Quick Start, с него можно начать и продолжить уже по аналогии).\n",
    "- [`tensorflow object detection api`](https://github.com/tensorflow/models/tree/master/research/object_detection): можно использовать TF ObjDet API. Чтобы было проще начать, стоит посмотреть [занятие предыдущего года](https://www.youtube.com/watch?v=xHIzyrU1uVM). Там есть Mask-RCNN, его можно использовать для сегментации\n",
    "- можно реалзиовать U-Net с нуля по [занятию Артура прошлого года](https://www.youtube.com/watch?v=OWK8VlgJM4I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "omWHJ712JvPf"
   },
   "source": [
    "Рекомендую выбрать один из них и работать уже с этим модулем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aDs5oSNmJvPg"
   },
   "source": [
    "> Результатом пункта является зафиксированный фреймворк для нейросети-сегментатора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iL2HHYXPJvPg"
   },
   "source": [
    "**2). Запуск сегментации на случайных изображениях**\n",
    "\n",
    "Этот пункт просто про то, чтобы запустить любую модель сегментации в выбранном выше репозитории. Таким образом, часть с запуском будет работать, и далее уже можно приступать к основным сценариям."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kQRg_-sOJvPh"
   },
   "source": [
    "> Результатом пункта явлется набор изображений, на каоторых модель успешно отработала и результат сегментации виден и понятен."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Dq7j4ahJvPi"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>1 сценарий</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pyUbP0YVJvPi"
   },
   "source": [
    "В первом сценарии упор делается на разработку демо, в которое встривается нейросетевая модель для сегментации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "90yFmWjFJvPj"
   },
   "source": [
    "Если у Вас есть опыт веб- или мобильной разработки, то можете использовать именного его и работать в рамках привычных Вам инструментов. Главное, чтобы в итоге они позволяил встроить в себя нейросетевой сегментатор, на вход которому будут поступить картинки.\n",
    "\n",
    "Изображения на вход демо могут поступать с веб-камеры, из файлов, по ссылке или с камеры мобильного телефона -- способ может быть любой (на Ваш выбор). Демо должно показывать, что сегментатор успешно отрабатывает на поданных изображениях и выделяет нужные объекты. Оформлять красиво визуальную часть демо -- приветствуется, но не обязательно -- это может быть просто кнопка \"загрузить\" и показ фото до/после сегментации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_U0iy4G_JvPk"
   },
   "source": [
    "**3). Выбор фреймворка/библиотеки для разработки веб/мобильного демо**\n",
    "\n",
    "***Основным инстурментом для разработки веб-демо будет микрофреймворк*** **Flask**: [серия туторилов на русском](https://habr.com/ru/post/346306/).   \n",
    "Полезные ресуры, которые стоит посмотреть, чтобы успешно разработать веб-демо:\n",
    "- [курс по веб-раработке](https://www.youtube.com/playlist?list=PLzQrZe3EemP5KsgWGnmC0QrOzQqjg3Kd5), нас интересуют первые 7 видео в плейлисте. В частности, нужны видео по Flask, там очень хорошие обучалки параллельно с лектором\n",
    "- [исчерпывающий справочник по Flask (англ)](https://www3.ntu.edu.sg/home/ehchua/programming/webprogramming/Python3_Flask.html)\n",
    "- если нужен будет готовый пример: можно посмотреть мой [репозиторий с реализацией веб-демо](https://github.com/izaharkin/Respalyzer) для ML-задачи оценки отзывов\n",
    "\n",
    "***Для разработки мобильного демо стоит выбрать инструмент на свое усмотрение***:\n",
    "- под Android: [пример реализации мобильного демо с помощью Android Studio](https://towardsdatascience.com/object-detector-android-app-using-pytorch-mobile-neural-network-407c419b56cd)\n",
    "- под iOS: можно использовать [Swift for TensorFlow](https://www.tensorflow.org/swift) или [CoreML](https://developer.apple.com/documentation/coreml)\n",
    "\n",
    "Разумеется, лучше ещё самостоятельно поискать видео/статьи на тему использования моделей на мобильных устройствах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTXEoaDUJvPk"
   },
   "source": [
    "> Результатом пункта является зафисированный для вас инстурмент для разработки демо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZOiHfb8jJvPl"
   },
   "source": [
    "**4). Разработка демо**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rcg8LlJsJvPm"
   },
   "source": [
    "Этот пункт про сам процесс написания кода для демо. \n",
    "\n",
    "> Результатом пункта является код, который можно запустить. Не хватать будет только логики сегментатора, сам интерфейс должен быть уже рабочим."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BkGTEr__JvPm"
   },
   "source": [
    "**5). Встраивание модели-сегментатора в демо**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P_V6fN4xJvPn"
   },
   "source": [
    "Этот пункт про процесс дописывания кода, который будет обеспечивать \"логику\" демо -- сама сегментация.\n",
    "\n",
    "> Результатом пункта является код, который можно запустить и продемонстрировать работающую систему сегментации объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "70aHzDArJvPn"
   },
   "source": [
    "**6). Тестирование демо**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I0Wc41KQJvPo"
   },
   "source": [
    "Здесь нужно запустить ваше демо на как можно большем количестве примеров, чтобы понять, в чем его сильные и слабые стороны. То есть какие объекты/сцены нейросеть обрабатывает легко, а с какими ему справиться сложно. Нужно предложить также пути для улучшения модели на основе увиденных ошибок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EAlDZtcEJvPp"
   },
   "source": [
    "> Результатом пункта является набор изображений, на которых демо отработало. Для каждого изображения нужно добавить комментарии, почему модель справилась хорошо/плохо, предложить пути ее улучшения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9_4-dV4pJvPp"
   },
   "source": [
    "**7). Оформление демо для показа другим людям**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XUFcp8geJvPq"
   },
   "source": [
    "В этом пункте можно пойти двумя путями:\n",
    "1. Проделать работу по улучшению визуальной составляющей демо (интерфейс)\n",
    "2. Загрузить модель на какой-нибудь сервер/хост/test-flight (в случае мобильного iOS-демо), чтобы к демо можно было обратиться прямо в адресной строке браузера\n",
    "\n",
    "По *первому пункту* могу посоветовать использовать библиотеку [Bootstrap](https://habr.com/ru/post/349060/), для мобильного демо элементы UI/UX являются частью основной разработки (поэтому стоит просто погуглить/почитать дщокументацию).\n",
    "\n",
    "По *второму пункту*: [быстрый способ задеплоить демо на Google Cloud Engine](https://www.youtube.com/watch?v=QUYCiIkzZlA). Если с GCE проблемы/не хочется привязывать карту и т.д., могут помочь эти ресурсы и сервис [Heroku](https://www.heroku.com/):\n",
    "- [в целом про деплой](http://www.tutorialspoint.com/flask/flask_deployment.htm)\n",
    "- [туториал 1](https://www.youtube.com/watch?v=pmRT8QQLIqk)\n",
    "- [туториал 2](https://medium.com/the-andela-way/deploying-your-flask-application-to-heroku-c99050bce8f9)\n",
    "- [выбрать своё собственное доменное имя на Heroku](https://devcenter.heroku.com/articles/custom-domains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJP8OnzsJvPs"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>2 сценарий</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3viJ2YF1JvPs"
   },
   "source": [
    "Во втором сценарии упор делается на сбор выборки, преобработку данных, обучение модели и измерение качества её работы (то есть осуществляется полный цикл разработки нейросетевой модели для задачи сегментации)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dag_q-SlJvPt"
   },
   "source": [
    "**3). Выбор датасета**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jvSoOMB1JvPu"
   },
   "source": [
    "При работе с датасетом вы неизбежно столкнетесь с работой с файлами и папками (директориями). Рекомендуется освежить в памяти работу с библиотеками `os`, `json`, `glob`. Может помочь [этот туториал](https://realpython.com/working-with-files-in-python/).\n",
    "\n",
    "По-умолчанию модели из `torchvision.models.segmentation` обучены на [MS COCO](http://cocodataset.org/#home) или [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/) датасете. Рекомендуется про них тоже почитать по приведенным ссылкам, потому что это крайне часто используемые в зрении наборы данных.\n",
    "\n",
    "На выбор предоставляются датасеты для сегментации объектов:\n",
    "1. [Семантическая и instance-сегментация датасета Cityscapes](https://www.cityscapes-dataset.com/downloads/): известный датасет для сегментации машин, пешеходов и вообще элементов обычных улиц в целом. Ознакомиться с примерами разметки можно [здесь](https://www.cityscapes-dataset.com/examples/#fine-annotations). Если для скачивания датасета Ваша почта не подходит (нужен корпоративный аккаунт), можно скачать его [этой ссылке](https://drive.google.com/open?id=1ht502uaDQmQYNoTMnRwLyn4BVluklwP1). Для предобработки данных могут пригодиться [эти скрипты](https://github.com/mcordts/cityscapesScripts). Также у MIT-курса по DL есть [занятие на Cityscapes](https://github.com/lexfridman/mit-deep-learning/blob/master/tutorial_driving_scene_segmentation/tutorial_driving_scene_segmentation.ipynb), можно посмотреть его;\n",
    "2. [Семантическая сегментация автомобилей из фона](https://www.kaggle.com/c/carvana-image-masking-challenge/data): чтобы было проще начать предобработку данных, можно посмотреть [этот ноутбук с соревнования](https://www.kaggle.com/vfdev5/data-visualization);\n",
    "3. [Семантическая сегментация типов поверхностей (материалов, из которых они сделаны)](http://opensurfaces.cs.cornell.edu/publications/opensurfaces/): на том же сайте есть ссылки на документацию, которая определнно будет полезна при предобработке;\n",
    "4. [Семантическая сегментация ядер в биологических клетках](https://www.kaggle.com/c/data-science-bowl-2018/data): стоит посмотреть [этот ноутбук с соревнования](https://www.kaggle.com/stkbailey/teaching-notebook-for-total-imaging-newbies), чтобы уметь правильно предобрабатывать данные;\n",
    "5. [Instance-сегментация одежды](https://github.com/switchablenorms/DeepFashion2): стоит прочитать README на главной странице репозитория. Для получения датасета нужно запросить пароль у автора через гугл-форму. После скачивания распакуйте его с использованием пароля. Из файлов аннотаций нас будут интересовать только `segmentation`, `category_name` и `category_id`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x398dPeLJvPu"
   },
   "source": [
    "> Результатом выполнения пункта явлется загруженный датасет, состоящий из изображений и разметки к ним (масок всех объектов на каждом изображении)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vBs5hEtlJvPv"
   },
   "source": [
    "**4). Предобработка данных**\n",
    "\n",
    "Самый непростой этап в этом сценарии. Скачать данные -- лишь половина дела. Чтобы обучить нейросеть на этих данных, нужно написать генератор батчей. Однако если будем подавать изображения так, как они есть, то даже батч собрать не сможем -- нужно привести их к однмоу размеру. Далее нужно привести их к типу float, переместить на CUDA и поделить на 255 (подробнее см. [занятие](https://www.youtube.com/watch?v=XSPYe4-y4HE)). Также нужно настроить аугментации и постобработку.\n",
    "\n",
    "То, как именно все это будет реализоваться -- зависит от инструмента, выбранного в пункте 1. Например, в detectron2 в обучалках есть то, какими на вход принимаются данные для обучения. Возможно, нужно будет зайти в документацию и почитать более подроюно, чтобы разобраться, какой именно формат масок принимает на вход сеть для сегментации при обучении.\n",
    "\n",
    "НЕ нужно копировать все файлы с картинками и разметкой прямо на диске в их предобработанные версии. Хороший тон -- осуществлять всю эту обработку программно, \"на лету\". То есть в идеале вы прописываете тот самый `DataLoader`, который на вход берет пути к изоборажениям и файлам разметки, а возвращать будет батчи нужного размера с уже обработанными числовыми тензорами изобраений и боксов. Можно реализовать эти функции по аналогии с теми, что есть в [Catalyst](https://github.com/catalyst-team/catalyst), который был рассмотрен на занятии.\n",
    "\n",
    "> Результатом выполнения пункта явлется код, запуск которого ведет к подаче батчей правильного вида (разметка приведена к требуемому формату масок, изображения нужного типа, размера и поделены на 255 и т.д.) для обучения нейронной сети для сегментации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7UhBstMfJvPv"
   },
   "source": [
    "**5). Обучение модели для сегментации** \n",
    "\n",
    "Стоит написать цикл обучения на PyTorch самостоятельно. Однако запрещать пользоваться сторонними библиотеками для обучения я не буду: можно использовать [Catalyst](https://github.com/catalyst-team/catalyst), [PyTorch Lightning](https://github.com/williamFalcon/pytorch-lightning) или [Ignite](https://github.com/pytorch/ignite). То есть саму модель можно взять из фреймворка из пункта 1, и само обучение осуществлять с помощью одной перечисленных трех библиотек (фреймворков)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kmw6vy4MJvPw"
   },
   "source": [
    "> Результатом выполнения пункта явлется код, запуск которого ведет к обучению модели на выбранном датасете. При обучении **обязателньо выводить числовые значения лосса на трейне и валидации**, крайне желательно использовать [`TensorBoard`](https://pytorch.org/docs/stable/tensorboard.html) для визуализации. Обязательно также сохранять модель после каждой N-ой эпохи, чтобы потом ее качество можно было проверить и веса были переиспользуемыми."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_3h8kF3bJvPw"
   },
   "source": [
    "**6). Измерения качества работы модели (метрики согласуются с руководителем и зависят от задачи)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OYEgGG8LJvPx"
   },
   "source": [
    "Под метриками понимаются функции/формулы, по которым оценивается качество модели для сегментации. Обычно для измерения качества работы нейросети для сегментации используют **IoU** и **попиксельную Accuracy** для каждого класса по-отдельности. Подробно метрики можно почитать [здесь](https://www.jeremyjordan.me/evaluating-image-segmentation-models/), также **обязательная к прочетнию [часть статьи про DICE loss](https://www.jeremyjordan.me/semantic-segmentation/#loss)** (а лучше и вовсе рочитать всю статью полностью).\n",
    "\n",
    "В зависимости от датасета и задачи (семантическая или instance) метрика может быть или IoU, или попиксельный Accuracy, или сам DICE. Мы по-умолчанию будем использовать IoU, её и стоит реализовать для **семантической сегментации** (по-желанию можно реализовать и остальные метрики). Для **instance-сегментации** стоит выводить precision и recall для каждого класса, зафиксировав порог IoU, после которого сегментация считается \"попавшей\" в Groung Truth разметку.\n",
    "\n",
    "Необходимо самостоятельно реализовать метрики для задачи сегментации. На вход функциям метрик поступают веса модели и выборка, на которой метрики нужно измерить. На выходе -- таблица с метриками для каждого класса.\n",
    "\n",
    "> Результат пункта -- реализованные функции метрик для задачи сегментации, позволяющие оценить качество работы модели на выборке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QMApjgLUJvPy"
   },
   "source": [
    " **7). Поиск путей применения этой модели в бизнесе/реальных задачах/набросок встраивания в веб/мобильное демо**\n",
    " \n",
    "В этом пункте нужно подумать, как эта модель может быть использована в дальнейшем. То есть, например, зачем нужно сегментировать автомобили на дороге? Или одежду?\n",
    "\n",
    "> Результат пункта -- перечисленные кейсы использования модели (описанные **как можно подробнее**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CsSU_GNEJvPy"
   },
   "source": [
    "<h2 style=\"text-align: center;\"><b>Критерии оценивания</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vi3QcAIHJvPz"
   },
   "source": [
    "* 1-2 пункты -- по 1 баллу  \n",
    "* 3 пункт -- 0 баллов (промежуточный пункт)  \n",
    "* 4 пункт -- 3 балла  \n",
    "* 5 пункт -- 3 балла  \n",
    "* 6-7 пункты -- по 1 баллу  \n",
    "* Макс баллов по проекту = 10  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6xM2k3IHJvPz"
   },
   "source": [
    "**Успехов в выполнении проекта!** \n",
    "\n",
    "Желаю всем проделать полезную, интересную и качественную работу, которую потом нестыдно и в резюме указать, и друзьям показать ;)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[project]segmentation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
